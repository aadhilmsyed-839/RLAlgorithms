{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e321480-5a2c-4e53-b727-c6ff94e0563f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Reinforcement Learning Algoirthms\n",
    "from stable_baselines3 import A2C, DDPG, DQN, HER, PPO, SAC, TD3\n",
    "from sb3_contrib import ARS, MaskablePPO, QRDQN, RecurrentPPO, TQC, TRPO\n",
    "\n",
    "# Import Logger and Recorder\n",
    "from stable_baselines3.common.logger import configure\n",
    "from stable_baselines3.common.vec_env import VecVideoRecorder\n",
    "\n",
    "# Import OpenAI Gym & Other Important Libraries\n",
    "import gymnasium as gym\n",
    "import time\n",
    "from typing import Type\n",
    "\n",
    "# Import Data Analysis & Visualization Libraies\n",
    "import numpy  as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c57d3b2-d53c-4518-bfed-7b1875a22529",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_algorithm(alg: Type['abc.ABCMeta'], env_name : str, timesteps : int):\n",
    "\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        This function takes an SB3 Reinforcement Learning Algorithm as a parameter \n",
    "        and trains the specified OpenAI Gym environment using this algoirthm. The \n",
    "        trained model is then executed and a video recording of the agent is saved.\n",
    "\n",
    "    Parameters:\n",
    "        - alg (abc.ABCMeta) : Indicates which algorithm to use for training\n",
    "        - env_name (str)    : Name of the Environment to test in\n",
    "        - timesteps (int)   : Number of Timesteps for the Learning Stage\n",
    "\n",
    "    Returns:\n",
    "        - timesteps (int)      : Number of Timesteps the trained agent performed during execution\n",
    "        - total_time (float)   : Number of Seconds the Training / Execution Took\n",
    "        - total_reward (float) : The Total Reward From the Execution Stage of the Trained Agent\n",
    "    \"\"\"\n",
    "\n",
    "    # Start Algorithm Timer\n",
    "    alg_start = time.time()\n",
    "    \n",
    "    # Create environment\n",
    "    env = None\n",
    "    try: env = gym.make(env_name, render_mode = \"rgb_array\")\n",
    "    except: raise ValueError(f\"Unknown Environment: {env_name}\")\n",
    "\n",
    "    # Instantiate the agent\n",
    "    model = None\n",
    "    try: model = alg(\"MlpPolicy\", env, verbose = 0)\n",
    "    except: raise ValueError(f\"Unknown Algorithm: {alg}\")\n",
    "\n",
    "    # Set up a new logger\n",
    "    print(\"Setting up Logger:\")\n",
    "    log_path = f'./{env_name}/{alg.__name__}/log/'\n",
    "    formats = [\"stdout\", \"csv\", \"tensorboard\"]\n",
    "    new_logger = configure(log_path, formats)\n",
    "    model.set_logger(new_logger)\n",
    "    \n",
    "    # Train the agent and display a progress bar\n",
    "    model.learn(total_timesteps = int(timesteps), progress_bar = True)\n",
    "    \n",
    "    # Initialize total reward and timesteps\n",
    "    total_rew : float = 0\n",
    "    timesteps : int   = 0\n",
    "    \n",
    "    # Create a vectorized environment for recording\n",
    "    vec_env = VecVideoRecorder(\n",
    "        venv                 = model.get_env(),\n",
    "        video_folder         = f'./{env_name}/{alg.__name__}/videos/',\n",
    "        record_video_trigger = 10_000,\n",
    "        video_length         = 10_000,\n",
    "    )\n",
    "    \n",
    "    # Execute the Trained Agent\n",
    "    obs = vec_env.reset()\n",
    "    while True:\n",
    "        action, _states = model.predict(obs, deterministic=True)\n",
    "        obs, rewards, done, info = vec_env.step(action)\n",
    "        vec_env.render(\"rgb_array\")\n",
    "        total_rew += rewards\n",
    "        timesteps += 1\n",
    "        if done: break\n",
    "    \n",
    "    # Close the vectorized Environment - Stops Recording\n",
    "    vec_env.close()\n",
    "\n",
    "    # Stop the Algorithm Timer\n",
    "    total_time = time.time() - alg_start\n",
    "\n",
    "    # Return the timesteps, execution time, and reward\n",
    "    return timesteps, total_time, total_rew[0]\n",
    "\n",
    "    # Delete Local Variables for Next Iteration\n",
    "    del model, env, vec_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707e4e81-4167-4232-b873-32d1755c77ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################\n",
    "# NON-CHANGEABLE PARAMETERS  \n",
    "#############################\n",
    "\n",
    "test_num = 1                       # Tracks the Test # of Current Iteration\n",
    "results_dict = {}                  # Stores the Result for each Algorithm\n",
    "it = 0                             # Iterator for the colors list\n",
    "\n",
    "# Colors for creating data visuals\n",
    "colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n",
    "\n",
    "\n",
    "#############################\n",
    "# CHANGEABLE PARAMETERS \n",
    "#############################\n",
    "\n",
    "env_name    = \"LunarLander-v2\"     # Name of the Environment to Run\n",
    "train_steps = 500_000              # Number of Timesteps for Learning\n",
    "\n",
    "# RL Algorithms for Testing\n",
    "rl_algs = [A2C, DQN, PPO, TRPO]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36cecb7-2c34-44b7-9dd5-3d1eac6b9287",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Start Test Counter & Start the Program Timer\n",
    "prog_start = time.time()\n",
    "\n",
    "# Test each RL algorithm with the defined parameters\n",
    "for alg in rl_algs:\n",
    "\n",
    "    # Program Status Output\n",
    "    print(\"=============================================\")\n",
    "    print(f\"Starting Test #{test_num}\")\n",
    "    print(f\"   Environment: {env_name}\")\n",
    "    print(f\"   Algorithm:   {alg.__name__}\")\n",
    "    print(f\"   Timesteps:   {int(train_steps)}\")\n",
    "    print(\"=============================================\")\n",
    "    \n",
    "    # Test the Algorithm with the defined parameters\n",
    "    total_timesteps, total_time, total_reward = test_algorithm(\n",
    "        alg       = alg,\n",
    "        env_name  = env_name,\n",
    "        timesteps = train_steps\n",
    "    )\n",
    "\n",
    "    # Add the Results to the Results Dictionary\n",
    "    results_dict[alg.__name__] = (f\"--------------------\\n\"\n",
    "            f\"The {alg.__name__} algorithm achieved a total reward of {total_reward:.2f} in {total_timesteps} timesteps.\\n\"\n",
    "            f\"The {alg.__name__} algorithm took {total_time:.2f} s ({(total_time/60):.2f} min) to execute.\\n\")\n",
    "\n",
    "    # Print the Results\n",
    "    print(results_dict[alg.__name__])\n",
    "\n",
    "    # Increment the Test Number to start new iteration\n",
    "    test_num += 1\n",
    "\n",
    "# Stop the Program Timer\n",
    "tot_time = time.time() - prog_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d9403a-02b1-4b10-bec3-6f50d1e4fb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the Results Dictionary HEADER\n",
    "print(\"\\n=============================================\")\n",
    "print(\"                Final Results                \")\n",
    "print(\"=============================================\\n\")\n",
    "\n",
    "for alg in rl_algs: \n",
    "    \n",
    "    # Print the Result of Each Function\n",
    "    print(f\"{alg.__name__}:\\n{results_dict[alg.__name__]}\\n\")\n",
    "\n",
    "    # Read the Progress CSV file for each algorithm\n",
    "    filename = f'./{env_name}/{alg.__name__}/log/progress.csv'\n",
    "    df = pd.read_csv(filename)\n",
    "\n",
    "    # Extract the Necessary Columns\n",
    "    x = df['time/total_timesteps']\n",
    "    y = df['rollout/ep_rew_mean']\n",
    "\n",
    "    # Add the Data to the Plot\n",
    "    plt.plot(x, y, label = alg.__name__, color = colors[it], linestyle='-', marker='o')\n",
    "    it = ((it + 1) % len(colors))\n",
    "\n",
    "# Add Labels and Title to the Plot\n",
    "plt.xlabel('Total Timesteps')\n",
    "plt.ylabel('Total Mean Reward')\n",
    "plt.title('Algorithm Performance for Lunar Lander Environment')\n",
    "\n",
    "# Display the Plot and its Legend\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Print the Termination Message\n",
    "print(\"\\n=============================================\")\n",
    "print(f\"This program took {(tot_time/60):.2f} mins to execute.\\n\")\n",
    "print(\"Terminating Program...\")\n",
    "print(\"=============================================\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
